{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal \n",
    "\n",
    "import pdb \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from mat4py import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Utilities.get_image_data import load_data\n",
    "from Utilities.form_train_val_test_batches import form_train_val_test_batches\n",
    "from Utilities.NN_FC_layerwise import FCLayerwise\n",
    "\n",
    "\n",
    "from Utilities.Net import Final_Network\n",
    "\n",
    "from Utilities.create_data import create_new\n",
    "\n",
    "from Utilities.L2_error_computation import error_L2\n",
    "from Utilities.loss_and_accuracies import data_loss_classification, data_loss_regression\n",
    "from Utilities.manifold_regularization import manifold_classification\n",
    "from Utilities.model_constraint import compute_interior_loss\n",
    "\n",
    "\n",
    "from Utilities.optimize_layerwise import optimize\n",
    "\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                       HyperParameters and RunOptions                        #\n",
    "###############################################################################\n",
    "class Hyperparameters:\n",
    "    max_hidden_layers = 7# For this architecture, need at least 2. One for the mapping to the feature space, one as a trainable hidden layer. EXCLUDES MAPPING BACK TO DATA SPACE\n",
    "    num_hidden_nodes  =100\n",
    "    activation        = 'elu'\n",
    "    classification_act= 'linear'\n",
    "    model_constraint  =  0.45\n",
    "    regularization    = 0.0001\n",
    "    manifold          = 0.001\n",
    "    node_TOL          = 1e-4\n",
    "    error_TOL         = 1e-4\n",
    "    batch_size        = 500\n",
    "    num_epochs        = 500\n",
    "    \n",
    "    num_networks      = 6\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunOptions:\n",
    "    def __init__(self):    \n",
    "        #=== Choose Which GPU to Use ===#\n",
    "        self.which_gpu = '1'\n",
    "        \n",
    "        #=== Use L_1 Regularization ===#\n",
    "        self.use_L1 = 1\n",
    "        \n",
    "        #=== Choose Data Set ===#\n",
    "        self.data_MNIST = 0\n",
    "        self.data_CIFAR10 = 0 \n",
    "        self.data_CIFAR100 = 0\n",
    "        self.data_regression=1\n",
    "        \n",
    "        #=== Random Seed ===#\n",
    "        self.random_seed = 1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                                 File Paths                                  #\n",
    "###############################################################################         \n",
    "class FilePaths():    \n",
    "    def __init__(self, hyperp, run_options):  \n",
    "        #=== Declaring File Name Components ===# \n",
    "        self.NN_type = 'FC'\n",
    "        if run_options.data_MNIST == 1:\n",
    "            self.dataset = 'MNIST'\n",
    "        if run_options.data_CIFAR10 == 1:\n",
    "            self.dataset = 'CIFAR10'\n",
    "        if run_options.data_CIFAR100 == 1:\n",
    "            self.dataset = 'CIFAR100'\n",
    "        if run_options.data_regression == 1:\n",
    "            self.dataset = 'Abalone'\n",
    "        if hyperp.regularization >= 1:\n",
    "            hyperp.regularization = int(hyperp.regularization)\n",
    "            regularization_string = str(hyperp.regularization)\n",
    "        else:\n",
    "            regularization_string = str(hyperp.regularization)\n",
    "            regularization_string = 'pt' + regularization_string[2:]                        \n",
    "        node_TOL_string = str('%.2e' %Decimal(hyperp.node_TOL))\n",
    "        node_TOL_string = node_TOL_string[-1]\n",
    "        error_TOL_string = str('%.2e' %Decimal(hyperp.error_TOL))\n",
    "        error_TOL_string = error_TOL_string[-1]\n",
    "        \n",
    "        #=== File Name ===#\n",
    "        if run_options.use_L1 == 0:\n",
    "            self.filename = self.dataset + '_' + self.NN_type + '_mhl%d_hl%d_eTOL%s_b%d_e%d' %(hyperp.max_hidden_layers, hyperp.num_hidden_nodes, error_TOL_string, hyperp.batch_size, hyperp.num_epochs)\n",
    "        else:\n",
    "            self.filename = self.dataset + '_' + self.NN_type + '_L1_mhl%d_hl%d_r%s_nTOL%s_eTOL%s_b%d_e%d' %(hyperp.max_hidden_layers, hyperp.num_hidden_nodes, regularization_string, node_TOL_string, error_TOL_string, hyperp.batch_size, hyperp.num_epochs)\n",
    "\n",
    "        #=== Saving Trained Neural Network and Tensorboard ===#\n",
    "        #self.NN_savefile_directory = 'C:/Users/Chandradut/Desktop/Sparse training/Trained_NNs/' + self.filename # Since we need to save four different types of files to save a neural network model, we need to create a new folder for each model\n",
    "        self.NN_savefile_directory =  self.filename\n",
    "        self.NN_savefile_name = self.NN_savefile_directory + '/' + self.filename # The file path and name for the four files\n",
    "        #self.tensorboard_directory = 'C:/Users/Chandradut/Desktop/Sparse training/Tensorboard/' + self.filename\n",
    "\n",
    "###############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":     \n",
    "\n",
    "    trainable=7\n",
    "    #=== Hyperparameters and Run Options ===#    \n",
    "    \n",
    "    hyperp = Hyperparameters()\n",
    "\n",
    "    run_options = RunOptions()\n",
    "    \n",
    "\n",
    "    #=== File Names ===#\n",
    "    file_paths = FilePaths(hyperp, run_options)\n",
    "    \n",
    "    #=== Load Data ===#       \n",
    "    data_train, labels_train,\\\n",
    "    data_test, labels_test,\\\n",
    "    data_input_shape, num_channels, label_dimensions, data_train_inside, labels_train_inside\\\n",
    "    = load_data(file_paths.NN_type, file_paths.dataset, run_options.random_seed) \n",
    "    \n",
    "    gauss_solution=np.loadtxt(\"gauss_solution.data\")\n",
    "    gauss_solution = tf.cast(gauss_solution,tf.float32)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    gauss_points_new = loadmat('gauss_points_new.mat')\n",
    "    gauss_points_new=np.array(list(gauss_points_new.values()))[-1]\n",
    "    gauss_points_new = tf.cast(gauss_points_new,tf.float32)\n",
    "    \n",
    "\n",
    "    \n",
    "    gauss_weights_new = loadmat('gauss_weights_new.mat')\n",
    "    gauss_weights_new=np.array(list(gauss_weights_new.values()))[-1].squeeze(-1)\n",
    "\n",
    "    \n",
    "    Coordinates = loadmat('Coordinates.mat')\n",
    "    Coordinates=np.array(list(Coordinates.values()))[-1]\n",
    "    Coordinates = tf.cast(Coordinates,tf.float32)\n",
    "    \n",
    "    \n",
    "    Stiffness = loadmat('Stiffness.mat')\n",
    "    Stiffness=np.array(list(Stiffness.values()))[-1]\n",
    "    Stiffness = tf.cast(Stiffness,tf.float32)\n",
    "    \n",
    "    load = loadmat('observation.mat')\n",
    "    load=np.array(list(load.values()))[-1]\n",
    "    load=tf.cast(load,tf.float32)\n",
    "    load=tf.reshape(load,(len(load),1))\n",
    "    \n",
    "    Solution = loadmat('Solution.mat')\n",
    "    Solution=np.array(list(Solution.values()))[-1]\n",
    "    Solution=tf.cast(Solution,tf.float32)\n",
    "    Solution=tf.reshape(Solution,(len(Solution),1))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    #for i in range(1,hyperp.num_networks):\n",
    "    for i in range(1,2):\n",
    "\n",
    "        if i>1:\n",
    "            trainable=2\n",
    "\n",
    "    \n",
    "            \n",
    "        if trainable==2:\n",
    "        \n",
    "        \n",
    "        \n",
    "            #=== GPU Settings ===#\n",
    "            os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = run_options.which_gpu\n",
    "    \n",
    "            #=== Neural Network ===#\n",
    "            if run_options.use_L1 == 0:\n",
    "                kernel_regularizer = None\n",
    "                bias_regularizer = None  \n",
    "            else:\n",
    "                kernel_regularizer = tf.keras.regularizers.l1(hyperp.regularization)\n",
    "                bias_regularizer = tf.keras.regularizers.l1(hyperp.regularization)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            data_train,new_label,labels_train,load,Solution=create_new(data_train, labels_train,hyperp,run_options, data_input_shape, label_dimensions,i,load,Stiffness,Coordinates,Solution)\n",
    "        \n",
    "           \n",
    "        #=== Construct Validation Set and Batches ===# \n",
    "            data_and_labels_train, data_and_labels_val, data_and_labels_test,\\\n",
    "            num_data_train, num_data_val, num_data_test,\\\n",
    "            num_batches_train, num_batches_val, num_batches_test,data_and_labels_train_new\\\n",
    "            = form_train_val_test_batches(data_train, labels_train, \\\n",
    "                                      data_test, labels_test, \\\n",
    "                                      hyperp.batch_size, new_label, run_options.random_seed)\n",
    "        \n",
    "        \n",
    "        if i==1 and trainable==2:\n",
    "            NN = FCLayerwise(hyperp, run_options, data_input_shape, label_dimensions,kernel_regularizer, bias_regularizer)    \n",
    "                                               #\n",
    "###############################################################################\n",
    "        if trainable>2:\n",
    "            del NN\n",
    "            NN = Final(hyperp, run_options, data_input_shape, label_dimensions,kernel_regularizer, bias_regularizer,trainable)   \n",
    "            #NN._set_inputs(data_train)\n",
    "            NN.load_weights(\"WEIGHTS\"+'/'+\"model_weights\"+str(1)+str(trainable-1))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        if i==1:\n",
    "            hyperp_n=hyperp\n",
    "            optimize(hyperp,hyperp_n, run_options, file_paths, NN, data_loss_classification, data_loss_regression, data_and_labels_train, data_and_labels_val, data_and_labels_test, label_dimensions, num_batches_train,data_and_labels_train_new,manifold_classification,hyperp.batch_size,run_options.random_seed,num_data_train,i,data_input_shape,data_train,labels_train,trainable,compute_interior_loss,error_L2,gauss_solution,gauss_points_new,gauss_weights_new,Coordinates, Stiffness, load,Solution,data_train_inside, labels_train_inside)   \n",
    "        \n",
    "      \n",
    "     \n",
    "        if not os.path.exists(\"WEIGHTS\"):\n",
    "            os.makedirs(\"WEIGHTS\")\n",
    "        NN.save_weights(\"WEIGHTS\"+'/'+\"model_weights\"+str(1)+str(trainable))\n",
    "        \n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRUE SOLUTION\n",
    "\n",
    "from Utilities.Net import Final_Network\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_plot_data=np.loadtxt(\"input_plot_data.data\")\n",
    "output_plot_data=np.loadtxt(\"output_plot_data\")\n",
    "\n",
    "\n",
    "        \n",
    "x = input_plot_data[:,0]\n",
    "y = input_plot_data[:,1]\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y = np.linspace(0, 1, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "Z=tf.reshape(output_plot_data,np.shape(X))\n",
    "\n",
    "fig = plt.figure()       \n",
    "\n",
    "#clevels = np.linspace(0.0019333754, 14.727704, 10000)\n",
    "levels=np.linspace(-0.5, 5, 1000)\n",
    "cs=plt.contourf(X, Y, Z,levels, cmap=cm.jet,vmax=5, vmin=-0.5)        \n",
    "#plt.contourf(X, Y, Z, 1000, cmap='RdYlBu_r', vmax=14.727704, vmin=0.) \n",
    "plt.colorbar() \n",
    "\n",
    "for c in cs.collections:\n",
    "    c.set_rasterized(True)\n",
    "\n",
    "\n",
    "plt.savefig('true_solution_simple.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NETWORK SOLUTION\n",
    "\n",
    "from Utilities.Net import Final_Network\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "plot_layer=7\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y = np.linspace(0, 1, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "d = []\n",
    "for i in range(0,1000):\n",
    "    for j in range(0,1000):\n",
    "        dd=[X[i,j],Y[i,j]]\n",
    "        d.append(dd) \n",
    "\n",
    "d=np.array(d)\n",
    "data= tf.cast(d,tf.float32)\n",
    "\n",
    "hyperp.max_hidden_layers=plot_layer+1\n",
    "\n",
    "Network=Final_Network(hyperp,run_options, data_input_shape, label_dimensions) \n",
    "        \n",
    "Network.load_weights(\"WEIGHTS\"+'/'+\"model_weights\"+str(1)+str(plot_layer)).expect_partial()\n",
    "    \n",
    "y_pred_test_add=Network(data)\n",
    "\n",
    "        \n",
    "fig = plt.figure()       \n",
    "Z=tf.reshape(y_pred_test_add,np.shape(X))\n",
    "#clevels = np.linspace(0.0019333754, 14.727704, 10000)\n",
    "  \n",
    "levels=np.linspace(-1.6, 8, 1000)\n",
    "cs=plt.contourf(X, Y, Z,levels, cmap=cm.jet,vmax=8, vmin=-1.6)  \n",
    "#plt.contourf(X, Y, Z, 1000, cmap='RdYlBu_r', vmax=14.727704, vmin=0.) \n",
    "plt.colorbar() \n",
    "\n",
    "for c in cs.collections:\n",
    "    c.set_rasterized(True)\n",
    "\n",
    "\n",
    "plt.savefig('sixth_solution_PRNN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6196c5e23c6b2b379c4220643bf9259826535d9ad3f9f06e52707346ee557ded"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

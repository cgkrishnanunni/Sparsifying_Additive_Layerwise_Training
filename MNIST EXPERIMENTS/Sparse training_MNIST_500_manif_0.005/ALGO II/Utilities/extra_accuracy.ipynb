{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utilities.get_image_data import load_data\n",
    "from Utilities.form_train_val_test_batches import form_train_val_test_batches\n",
    "from Utilities.NN_FC_layerwise import FCLayerwise\n",
    "from Utilities.NN_FC_layerwise_new import FCLayerwise_new\n",
    "from Utilities.Net import Final_Network\n",
    "from Utilities.Net_new import Final_Network_ALGO_II\n",
    "from Utilities.create_data import create_new\n",
    "from Utilities.loss_and_accuracies import data_loss_classification, accuracy_classification\n",
    "from Utilities.manifold_regularization import manifold_classification\n",
    "from Utilities.manifold_regularization_new import manifold_classification_new\n",
    "from Utilities.optimize_layerwise import optimize\n",
    "from Utilities.additive_output import net_output \n",
    "from Utilities.EXTRA_ACCURATE_NETWORK_OPTIMIZE import optimize_new\n",
    "from Utilities.EXTRA_ACCURATE_NETWORK import FCLayerwise_extra_accurate\n",
    "from Utilities.plot_and_save_figures_layerwise import plot_fig\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal # for filenames\n",
    "\n",
    "import pdb #Equivalent of keyboard in MATLAB, just add \"pdb.set_trace()\"\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                       HyperParameters and RunOptions                        #\n",
    "###############################################################################\n",
    "class Hyperparameters:\n",
    "    max_hidden_layers = 9 # For this architecture, need at least 2. One for the mapping to the feature space, one as a trainable hidden layer. EXCLUDES MAPPING BACK TO DATA SPACE\n",
    "    filter_size       = 3 \n",
    "    num_filters       = 64\n",
    "    activation        ='elu'\n",
    "    classification_act= 'linear'\n",
    "    regularization    = 0.001\n",
    "    manifold          = 0.01\n",
    "    node_TOL          = 1e-4\n",
    "    error_TOL         = 1e-4\n",
    "    batch_size        = 1000\n",
    "    num_epochs        = 40\n",
    "    \n",
    "    num_networks      = 6\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters_new:\n",
    "    max_hidden_layers = 3 # For this architecture, need at least 2. One for the mapping to the feature space, one as a trainable hidden layer. EXCLUDES MAPPING BACK TO DATA SPACE\n",
    "    num_hidden_nodes  = 100\n",
    "    activation        = 'elu'\n",
    "    classification_act= 'linear'\n",
    "    regularization    = 0.000\n",
    "    manifold          = 0.000\n",
    "    node_TOL          = 1e-4\n",
    "    error_TOL         = 1e-4\n",
    "    batch_size        = 1000\n",
    "    num_epochs        = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunOptions:\n",
    "    def __init__(self):    \n",
    "        #=== Choose Which GPU to Use ===#\n",
    "        self.which_gpu = '1'\n",
    "        \n",
    "        #=== Use L_1 Regularization ===#\n",
    "        self.use_L1 = 1\n",
    "        \n",
    "        #=== Choose Data Set ===#\n",
    "        self.data_MNIST = 0\n",
    "        self.data_CIFAR10 = 1 \n",
    "        self.data_CIFAR100 = 0\n",
    "        \n",
    "        #=== Random Seed ===#\n",
    "        self.random_seed = 1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                                 File Paths                                  #\n",
    "###############################################################################         \n",
    "class FilePaths():    \n",
    "    def __init__(self, hyperp, run_options):  \n",
    "        #=== Declaring File Name Components ===# \n",
    "        self.NN_type = 'CNN'\n",
    "        if run_options.data_MNIST == 1:\n",
    "            self.dataset = 'MNIST'\n",
    "        if run_options.data_CIFAR10 == 1:\n",
    "            self.dataset = 'CIFAR10'\n",
    "        if run_options.data_CIFAR100 == 1:\n",
    "            self.dataset = 'CIFAR100'\n",
    "        if hyperp.regularization >= 1:\n",
    "            hyperp.regularization = int(hyperp.regularization)\n",
    "            regularization_string = str(hyperp.regularization)\n",
    "        else:\n",
    "            regularization_string = str(hyperp.regularization)\n",
    "            regularization_string = 'pt' + regularization_string[2:]                        \n",
    "        node_TOL_string = str('%.2e' %Decimal(hyperp.node_TOL))\n",
    "        node_TOL_string = node_TOL_string[-1]\n",
    "        error_TOL_string = str('%.2e' %Decimal(hyperp.error_TOL))\n",
    "        error_TOL_string = error_TOL_string[-1]\n",
    "        \n",
    "        #=== File Name ===#\n",
    "        if run_options.use_L1 == 0:\n",
    "            self.filename = self.dataset + '_' + self.NN_type + '_mhl%d_hl%d_eTOL%s_b%d_e%d' %(hyperp.max_hidden_layers, hyperp.num_filters, error_TOL_string, hyperp.batch_size, hyperp.num_epochs)\n",
    "        else:\n",
    "            self.filename = self.dataset + '_' + self.NN_type + '_L1_mhl%d_hl%d_r%s_nTOL%s_eTOL%s_b%d_e%d' %(hyperp.max_hidden_layers, hyperp.num_filters, regularization_string, node_TOL_string, error_TOL_string, hyperp.batch_size, hyperp.num_epochs)\n",
    "\n",
    "        #=== Saving Trained Neural Network and Tensorboard ===#\n",
    "        #self.NN_savefile_directory = 'C:/Users/Chandradut/Desktop/Sparse training/Trained_NNs/' + self.filename # Since we need to save four different types of files to save a neural network model, we need to create a new folder for each model\n",
    "        self.NN_savefile_directory =  self.filename\n",
    "        self.NN_savefile_name = self.NN_savefile_directory + '/' + self.filename # The file path and name for the four files\n",
    "        #self.tensorboard_directory = 'C:/Users/Chandradut/Desktop/Sparse training/Tensorboard/' + self.filename\n",
    "\n",
    "###############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\chandradut\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py:1556: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "ListWrapper([ListWrapper([32, 3]), ListWrapper([3, 64]), ListWrapper([3, 64]), 10])\n",
      "Beginning Training\n",
      "================================\n",
      "            Epoch 0            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Model: \"fc_layerwise_extra_accurate\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "W2 (Conv2D)                  multiple                  36928     \n",
      "_________________________________________________________________\n",
      "W3 (Conv2D)                  multiple                  36928     \n",
      "_________________________________________________________________\n",
      "W4 (Conv2D)                  multiple                  36928     \n",
      "_________________________________________________________________\n",
      "W5 (Conv2D)                  multiple                  36928     \n",
      "_________________________________________________________________\n",
      "W6 (Conv2D)                  multiple                  36928     \n",
      "_________________________________________________________________\n",
      "W7 (Conv2D)                  multiple                  36928     \n",
      "_________________________________________________________________\n",
      "W8 (Conv2D)                  multiple                  36928     \n",
      "_________________________________________________________________\n",
      "upsampling_layer (Conv2D)    multiple                  9472      \n",
      "_________________________________________________________________\n",
      "maxpool_layer_new (MaxPoolin multiple                  0         \n",
      "_________________________________________________________________\n",
      "maxpool_layer (AveragePoolin multiple                  0         \n",
      "_________________________________________________________________\n",
      "classification_layer (Dense) multiple                  10250     \n",
      "=================================================================\n",
      "Total params: 278,218\n",
      "Trainable params: 278,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Time per Batch: 2.57\n",
      "Time per Epoch: 108.41\n",
      "\n",
      "Training Set: Loss: 2.069e+00, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.476\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 1            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.42\n",
      "Time per Epoch: 110.29\n",
      "\n",
      "Training Set: Loss: 1.376e+00, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.534\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 2            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.37\n",
      "Time per Epoch: 113.30\n",
      "\n",
      "Training Set: Loss: 1.217e+00, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.589\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 3            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.45\n",
      "Time per Epoch: 111.04\n",
      "\n",
      "Training Set: Loss: 1.085e+00, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.627\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 4            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.70\n",
      "Time per Epoch: 112.70\n",
      "\n",
      "Training Set: Loss: 1.018e+00, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.637\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 5            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.47\n",
      "Time per Epoch: 113.60\n",
      "\n",
      "Training Set: Loss: 9.365e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.645\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 6            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.68\n",
      "Time per Epoch: 113.21\n",
      "\n",
      "Training Set: Loss: 8.909e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.664\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 7            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.85\n",
      "Time per Epoch: 112.80\n",
      "\n",
      "Training Set: Loss: 8.316e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.678\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 8            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.52\n",
      "Time per Epoch: 113.11\n",
      "\n",
      "Training Set: Loss: 7.868e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.686\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 9            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.88\n",
      "Time per Epoch: 112.66\n",
      "\n",
      "Training Set: Loss: 7.477e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.692\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 10            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.82\n",
      "Time per Epoch: 117.00\n",
      "\n",
      "Training Set: Loss: 7.102e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.699\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 11            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.96\n",
      "Time per Epoch: 117.27\n",
      "\n",
      "Training Set: Loss: 6.749e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.706\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 12            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.77\n",
      "Time per Epoch: 115.24\n",
      "\n",
      "Training Set: Loss: 6.372e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.703\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 13            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per Batch: 2.56\n",
      "Time per Epoch: 114.54\n",
      "\n",
      "Training Set: Loss: 5.919e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.707\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 14            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.88\n",
      "Time per Epoch: 115.02\n",
      "\n",
      "Training Set: Loss: 5.696e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.719\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 15            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.83\n",
      "Time per Epoch: 114.90\n",
      "\n",
      "Training Set: Loss: 5.344e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.716\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 16            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.86\n",
      "Time per Epoch: 115.24\n",
      "\n",
      "Training Set: Loss: 5.016e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.730\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 17            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.89\n",
      "Time per Epoch: 116.36\n",
      "\n",
      "Training Set: Loss: 4.768e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.721\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 18            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.70\n",
      "Time per Epoch: 115.01\n",
      "\n",
      "Training Set: Loss: 4.624e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.715\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 19            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.72\n",
      "Time per Epoch: 113.24\n",
      "\n",
      "Training Set: Loss: 4.156e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.719\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 20            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.80\n",
      "Time per Epoch: 113.45\n",
      "\n",
      "Training Set: Loss: 3.978e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.731\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 21            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.93\n",
      "Time per Epoch: 114.55\n",
      "\n",
      "Training Set: Loss: 3.572e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.736\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 22            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.94\n",
      "Time per Epoch: 115.69\n",
      "\n",
      "Training Set: Loss: 3.149e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.733\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 23            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.52\n",
      "Time per Epoch: 116.09\n",
      "\n",
      "Training Set: Loss: 3.040e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.732\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 24            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.59\n",
      "Time per Epoch: 114.44\n",
      "\n",
      "Training Set: Loss: 2.697e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.730\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 25            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.55\n",
      "Time per Epoch: 115.29\n",
      "\n",
      "Training Set: Loss: 2.501e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.733\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 26            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.55\n",
      "Time per Epoch: 121.34\n",
      "\n",
      "Training Set: Loss: 2.267e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.732\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 27            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.86\n",
      "Time per Epoch: 116.22\n",
      "\n",
      "Training Set: Loss: 2.023e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.730\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 28            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.59\n",
      "Time per Epoch: 122.17\n",
      "\n",
      "Training Set: Loss: 1.693e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.733\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 29            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.56\n",
      "Time per Epoch: 115.19\n",
      "\n",
      "Training Set: Loss: 1.581e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.726\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 30            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.58\n",
      "Time per Epoch: 118.15\n",
      "\n",
      "Training Set: Loss: 1.521e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.733\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 31            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per Batch: 2.55\n",
      "Time per Epoch: 115.69\n",
      "\n",
      "Training Set: Loss: 1.319e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.729\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 32            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.59\n",
      "Time per Epoch: 115.83\n",
      "\n",
      "Training Set: Loss: 1.278e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.725\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 33            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.52\n",
      "Time per Epoch: 117.91\n",
      "\n",
      "Training Set: Loss: 1.053e-01, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.728\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 34            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.62\n",
      "Time per Epoch: 116.05\n",
      "\n",
      "Training Set: Loss: 7.414e-02, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.732\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 35            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.66\n",
      "Time per Epoch: 114.95\n",
      "\n",
      "Training Set: Loss: 5.531e-02, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.736\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 36            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.58\n",
      "Time per Epoch: 116.04\n",
      "\n",
      "Training Set: Loss: 4.170e-02, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.733\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 37            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.55\n",
      "Time per Epoch: 115.08\n",
      "\n",
      "Training Set: Loss: 2.797e-02, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.736\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 38            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.53\n",
      "Time per Epoch: 115.82\n",
      "\n",
      "Training Set: Loss: 2.319e-02, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.739\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n",
      "================================\n",
      "            Epoch 39            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl64_rpt001_nTOL4_eTOL4_b1000_e40\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n",
      "Time per Batch: 2.59\n",
      "Time per Epoch: 115.47\n",
      "\n",
      "Training Set: Loss: 1.810e-02, Accuracy: 0.000\n",
      "Validation Set: Loss: 0.000e+00, Accuracy: 0.000\n",
      "Test Set: Loss: 0.000e+00, Accuracy: 0.741\n",
      "\n",
      "Previous Layer Relative # of 0s: 0.0000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    #=== Hyperparameters and Run Options ===#    \n",
    "    hyperp = Hyperparameters()\n",
    "    hyperp_new=Hyperparameters_new()\n",
    "    run_options = RunOptions()\n",
    "    \n",
    "\n",
    "    #=== File Names ===#\n",
    "    file_paths = FilePaths(hyperp, run_options)\n",
    "    \n",
    "   # === Load Data ===#       \n",
    "    data_train, labels_train,\\\n",
    "    data_test, labels_test,\\\n",
    "    data_input_shape, num_channels, label_dimensions\\\n",
    "    = load_data(file_paths.NN_type, file_paths.dataset, run_options.random_seed) \n",
    "            \n",
    "            #=== GPU Settings ===#\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = run_options.which_gpu\n",
    "    \n",
    "            #=== Neural Network ===#\n",
    "    if run_options.use_L1 == 0:\n",
    "        kernel_regularizer = None\n",
    "        bias_regularizer = None  \n",
    "    else:\n",
    "        kernel_regularizer = tf.keras.regularizers.l1(hyperp.regularization)\n",
    "        bias_regularizer = tf.keras.regularizers.l1(hyperp.regularization)\n",
    "\n",
    "    y_true = tf.one_hot(tf.cast(labels_train,tf.int64), label_dimensions, dtype=tf.float32)\n",
    "    new_label=np.squeeze(y_true,axis=1)\n",
    "    \n",
    "    data_and_labels_train, data_and_labels_val, data_and_labels_test,\\\n",
    "    num_data_train, num_data_val, num_data_test,\\\n",
    "    num_batches_train, num_batches_val, num_batches_test,data_and_labels_train_new\\\n",
    "    = form_train_val_test_batches(data_train, labels_train, \\\n",
    "                                data_test, labels_test, \\\n",
    "                                hyperp.batch_size, new_label, run_options.random_seed)\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "trainable_hidden_layer_index=8\n",
    "NN=FCLayerwise_extra_accurate( hyperp,run_options, data_input_shape, label_dimensions,trainable_hidden_layer_index) \n",
    "        \n",
    "#NN.load_weights(\"WEIGHTS_AFTER_ALGORITHM_I\"+'/'+\"model_weights\"+str(1)).expect_partial()\n",
    "hyperp_n=hyperp\n",
    "optimize_new(hyperp,hyperp_n, run_options, file_paths, NN, data_loss_classification, accuracy_classification, data_and_labels_train, data_and_labels_val, data_and_labels_test, label_dimensions, num_batches_train,data_and_labels_train_new,hyperp.batch_size,run_options.random_seed,num_data_train,1,data_input_shape)   \n",
    "           #=== Optimizer ===#\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CIFAR10_CNN_L1_mhl9_hl20_rpt001_nTOL4_eTOL4_b1000_e10/CIFAR10_CNN_L1_mhl9_hl20_rpt001_nTOL4_eTOL4_b1000_e10_metrics_hl21.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-67dbd3ab539a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_fig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\CIFAR_CNN\\Utilities\\plot_and_save_figures_layerwise.py\u001b[0m in \u001b[0;36mplot_fig\u001b[1;34m(hyperp, run_options, file_paths, i_val)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mdf_metrics\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0marray_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandradut\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandradut\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandradut\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandradut\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandradut\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CIFAR10_CNN_L1_mhl9_hl20_rpt001_nTOL4_eTOL4_b1000_e10/CIFAR10_CNN_L1_mhl9_hl20_rpt001_nTOL4_eTOL4_b1000_e10_metrics_hl21.csv'"
     ]
    }
   ],
   "source": [
    "plot_fig(hyperp, run_options, file_paths,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\chandradut\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py:1556: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "    #=== Hyperparameters and Run Options ===#    \n",
    "    hyperp = Hyperparameters()\n",
    "    hyperp_new=Hyperparameters_new()\n",
    "    run_options = RunOptions()\n",
    "    \n",
    "\n",
    "    #=== File Names ===#\n",
    "    file_paths = FilePaths(hyperp, run_options)\n",
    "    \n",
    "   # === Load Data ===#       \n",
    "    data_train, labels_train,\\\n",
    "    data_test, labels_test,\\\n",
    "    data_input_shape, num_channels, label_dimensions\\\n",
    "    = load_data(file_paths.NN_type, file_paths.dataset, run_options.random_seed) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.save_weights(\"WEIGHTS\"+'/'+\"model_weights\"+str(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "            #=== GPU Settings ===#\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = run_options.which_gpu\n",
    "    \n",
    "            #=== Neural Network ===#\n",
    "if run_options.use_L1 == 0:\n",
    "    kernel_regularizer = None\n",
    "    bias_regularizer = None  \n",
    "else:\n",
    "    kernel_regularizer = tf.keras.regularizers.l1(hyperp.regularization)\n",
    "    bias_regularizer = tf.keras.regularizers.l1(hyperp.regularization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "        data_and_labels_train, data_and_labels_val, data_and_labels_test,\\\n",
    "        num_data_train, num_data_val, num_data_test,\\\n",
    "        num_batches_train, num_batches_val, num_batches_test,data_and_labels_train_new\\\n",
    "        = form_train_val_test_batches(data_train, labels_train, \\\n",
    "                                  data_test, labels_test, \\\n",
    "                                  hyperp.batch_size, new_label, run_options.random_seed)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train,new_label,labels_train=create_new(0,data_train, labels_train,hyperp,hyperp_new, run_options, data_input_shape, label_dimensions,1)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper([ListWrapper([32, 3]), ListWrapper([3, 20]), ListWrapper([3, 20]), 10])\n",
      "Beginning Training\n",
      "================================\n",
      "            Epoch 0            \n",
      "================================\n",
      "CIFAR10_CNN_L1_mhl9_hl20_rpt001_nTOL4_eTOL4_b1000_e1\n",
      "Trainable Hidden Layer Index: 8\n",
      "GPU: 1\n",
      "\n",
      "Optimizing 40 batches of size 1000:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-25d58ac57462>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"WEIGHTS\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"model_weights\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpect_partial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhyperp_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0moptimize_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhyperp_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loss_classification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_classification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_and_labels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_and_labels_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_and_labels_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_dimensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_and_labels_train_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhyperp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_data_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m            \u001b[1;31m#=== Optimizer ===#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\CIFAR_CNN\\Utilities\\EXTRA_ACCURATE_NETWORK_OPTIMIZE.py\u001b[0m in \u001b[0;36moptimize_new\u001b[1;34m(hyperp, hyperp_new, run_options, file_paths, NN, data_loss, accuracy, data_and_labels_train, data_and_labels_val, data_and_labels_test, label_dimensions, num_batches_train, data_and_labels_train_new, batch_size, random_seed, num_data_train, i_val, data_input_shape)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                     \u001b[0mstart_time_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                     \u001b[0mbatch_pred_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_data_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m                     \u001b[1;31m#=== Display Model Summary ===#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "trainable_hidden_layer_index=8\n",
    "NN=FCLayerwise_extra_accurate( hyperp,run_options, data_input_shape, label_dimensions,trainable_hidden_layer_index) \n",
    "        \n",
    "NN.load_weights(\"WEIGHTS\"+'/'+\"model_weights\"+str(1)).expect_partial()\n",
    "hyperp_n=hyperp\n",
    "optimize_new(hyperp,hyperp_n, run_options, file_paths, NN, data_loss_classification, accuracy_classification, data_and_labels_train, data_and_labels_val, data_and_labels_test, label_dimensions, num_batches_train,data_and_labels_train_new,hyperp.batch_size,run_options.random_seed,num_data_train,1,data_input_shape)   \n",
    "           #=== Optimizer ===#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.save_weights(\"WEIGHTS\"+'/'+\"model_weights\"+str(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    file_paths = FilePaths(hyperp, run_options)\n",
    "    \n",
    "    #=== Load Data ===#       \n",
    "    data_train, labels_train,\\\n",
    "    data_test, labels_test,\\\n",
    "    data_input_shape, num_channels, label_dimensions\\\n",
    "    = load_data(file_paths.NN_type, file_paths.dataset, run_options.random_seed) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper([ListWrapper([32, 3]), ListWrapper([3, 20]), ListWrapper([3, 20]), 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2061a850c18>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            Network=Final_Network( hyperp,run_options, data_input_shape, label_dimensions) \n",
    "        \n",
    "            Network.load_weights(\"WEIGHTS\"+'/'+\"model_weights\"+str(2-1)).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb=Network(batch_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_data_test, batch_labels_test in data_and_labels_test:\n",
    "    b=batch_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.reshape(batch_data_test, (len(batch_data_test), 32*32*3))\n",
    "batch_pred_test,val = NN(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pred_test_new=batch_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnew=bb+batch_pred_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000,), dtype=float32, numpy=\n",
       "array([1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_classification(rnew, batch_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=Network(data_train[0:1000])\n",
    "for i in range(1000,50000,1000):\n",
    "    \n",
    "    r=Network(data_train[i:i+1000])\n",
    "    \n",
    "\n",
    "    y_pred=tf.concat([y_pred,r],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=Network(data_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.one_hot(tf.cast(labels_train,tf.int64), label_dimensions, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000, 1, 10), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true=np.squeeze(y_true,axis=1)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(y_true,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "(data_train, labels_train), (data_test, labels_test) = datasets.cifar10.load_data()\n",
    "#(data_train, labels_train), (data_test, labels_test) = datasets.mnist.load_data()\n",
    "#data_train = tf.reshape(data_train, (len(data_train), 28*28))\n",
    "#data_test = tf.reshape(data_test, (len(data_test), 28*28))\n",
    "#data_train = tf.reshape(data_train, (len(data_train), 32*32*3))\n",
    "#data_test = tf.reshape(data_test, (len(data_test), 32*32*3))\n",
    "    \n",
    "label_dimensions = 10\n",
    "data_input_shape = data_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000, 3072), dtype=uint8, numpy=\n",
       "array([[ 59,  62,  63, ..., 123,  92,  72],\n",
       "       [154, 177, 187, ..., 143, 133, 144],\n",
       "       [255, 255, 255, ...,  80,  86,  84],\n",
       "       ...,\n",
       "       [ 35, 178, 235, ...,  12,  31,  50],\n",
       "       [189, 211, 240, ..., 195, 190, 171],\n",
       "       [229, 229, 239, ..., 163, 163, 161]], dtype=uint8)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\chandradut\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py:1556: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "      #=== Hyperparameters and Run Options ===#    \n",
    "hyperp = Hyperparameters()\n",
    "hyperp_new=Hyperparameters_new()\n",
    "run_options = RunOptions()\n",
    "    \n",
    "\n",
    "    #=== File Names ===#\n",
    "file_paths = FilePaths(hyperp, run_options)\n",
    "    #=== Load Data ===#       \n",
    "data_train, labels_train,data_test, labels_test,data_input_shape, num_channels, label_dimensions= load_data(file_paths.NN_type, file_paths.dataset, run_options.random_seed) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 59,  62,  63],\n",
       "         [ 43,  46,  45],\n",
       "         [ 50,  48,  43],\n",
       "         ...,\n",
       "         [158, 132, 108],\n",
       "         [152, 125, 102],\n",
       "         [148, 124, 103]],\n",
       "\n",
       "        [[ 16,  20,  20],\n",
       "         [  0,   0,   0],\n",
       "         [ 18,   8,   0],\n",
       "         ...,\n",
       "         [123,  88,  55],\n",
       "         [119,  83,  50],\n",
       "         [122,  87,  57]],\n",
       "\n",
       "        [[ 25,  24,  21],\n",
       "         [ 16,   7,   0],\n",
       "         [ 49,  27,   8],\n",
       "         ...,\n",
       "         [118,  84,  50],\n",
       "         [120,  84,  50],\n",
       "         [109,  73,  42]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[208, 170,  96],\n",
       "         [201, 153,  34],\n",
       "         [198, 161,  26],\n",
       "         ...,\n",
       "         [160, 133,  70],\n",
       "         [ 56,  31,   7],\n",
       "         [ 53,  34,  20]],\n",
       "\n",
       "        [[180, 139,  96],\n",
       "         [173, 123,  42],\n",
       "         [186, 144,  30],\n",
       "         ...,\n",
       "         [184, 148,  94],\n",
       "         [ 97,  62,  34],\n",
       "         [ 83,  53,  34]],\n",
       "\n",
       "        [[177, 144, 116],\n",
       "         [168, 129,  94],\n",
       "         [179, 142,  87],\n",
       "         ...,\n",
       "         [216, 184, 140],\n",
       "         [151, 118,  84],\n",
       "         [123,  92,  72]]],\n",
       "\n",
       "\n",
       "       [[[154, 177, 187],\n",
       "         [126, 137, 136],\n",
       "         [105, 104,  95],\n",
       "         ...,\n",
       "         [ 91,  95,  71],\n",
       "         [ 87,  90,  71],\n",
       "         [ 79,  81,  70]],\n",
       "\n",
       "        [[140, 160, 169],\n",
       "         [145, 153, 154],\n",
       "         [125, 125, 118],\n",
       "         ...,\n",
       "         [ 96,  99,  78],\n",
       "         [ 77,  80,  62],\n",
       "         [ 71,  73,  61]],\n",
       "\n",
       "        [[140, 155, 164],\n",
       "         [139, 146, 149],\n",
       "         [115, 115, 112],\n",
       "         ...,\n",
       "         [ 79,  82,  64],\n",
       "         [ 68,  70,  55],\n",
       "         [ 67,  69,  55]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[175, 167, 166],\n",
       "         [156, 154, 160],\n",
       "         [154, 160, 170],\n",
       "         ...,\n",
       "         [ 42,  34,  36],\n",
       "         [ 61,  53,  57],\n",
       "         [ 93,  83,  91]],\n",
       "\n",
       "        [[165, 154, 128],\n",
       "         [156, 152, 130],\n",
       "         [159, 161, 142],\n",
       "         ...,\n",
       "         [103,  93,  96],\n",
       "         [123, 114, 120],\n",
       "         [131, 121, 131]],\n",
       "\n",
       "        [[163, 148, 120],\n",
       "         [158, 148, 122],\n",
       "         [163, 156, 133],\n",
       "         ...,\n",
       "         [143, 133, 139],\n",
       "         [143, 134, 142],\n",
       "         [143, 133, 144]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[113, 120, 112],\n",
       "         [111, 118, 111],\n",
       "         [105, 112, 106],\n",
       "         ...,\n",
       "         [ 72,  81,  80],\n",
       "         [ 72,  80,  79],\n",
       "         [ 72,  80,  79]],\n",
       "\n",
       "        [[111, 118, 110],\n",
       "         [104, 111, 104],\n",
       "         [ 99, 106,  98],\n",
       "         ...,\n",
       "         [ 68,  75,  73],\n",
       "         [ 70,  76,  75],\n",
       "         [ 78,  84,  82]],\n",
       "\n",
       "        [[106, 113, 105],\n",
       "         [ 99, 106,  98],\n",
       "         [ 95, 102,  94],\n",
       "         ...,\n",
       "         [ 78,  85,  83],\n",
       "         [ 79,  85,  83],\n",
       "         [ 80,  86,  84]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 35, 178, 235],\n",
       "         [ 40, 176, 239],\n",
       "         [ 42, 176, 241],\n",
       "         ...,\n",
       "         [ 99, 177, 219],\n",
       "         [ 79, 147, 197],\n",
       "         [ 89, 148, 189]],\n",
       "\n",
       "        [[ 57, 182, 234],\n",
       "         [ 44, 184, 250],\n",
       "         [ 50, 183, 240],\n",
       "         ...,\n",
       "         [156, 182, 200],\n",
       "         [141, 177, 206],\n",
       "         [116, 149, 175]],\n",
       "\n",
       "        [[ 98, 197, 237],\n",
       "         [ 64, 189, 252],\n",
       "         [ 69, 192, 245],\n",
       "         ...,\n",
       "         [188, 195, 206],\n",
       "         [119, 135, 147],\n",
       "         [ 61,  79,  90]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 73,  79,  77],\n",
       "         [ 53,  63,  68],\n",
       "         [ 54,  68,  80],\n",
       "         ...,\n",
       "         [ 17,  40,  64],\n",
       "         [ 21,  36,  51],\n",
       "         [ 33,  48,  49]],\n",
       "\n",
       "        [[ 61,  68,  75],\n",
       "         [ 55,  70,  86],\n",
       "         [ 57,  79, 103],\n",
       "         ...,\n",
       "         [ 24,  48,  72],\n",
       "         [ 17,  35,  53],\n",
       "         [  7,  23,  32]],\n",
       "\n",
       "        [[ 44,  56,  73],\n",
       "         [ 46,  66,  88],\n",
       "         [ 49,  77, 105],\n",
       "         ...,\n",
       "         [ 27,  52,  77],\n",
       "         [ 21,  43,  66],\n",
       "         [ 12,  31,  50]]],\n",
       "\n",
       "\n",
       "       [[[189, 211, 240],\n",
       "         [186, 208, 236],\n",
       "         [185, 207, 235],\n",
       "         ...,\n",
       "         [175, 195, 224],\n",
       "         [172, 194, 222],\n",
       "         [169, 194, 220]],\n",
       "\n",
       "        [[194, 210, 239],\n",
       "         [191, 207, 236],\n",
       "         [190, 206, 235],\n",
       "         ...,\n",
       "         [173, 192, 220],\n",
       "         [171, 191, 218],\n",
       "         [167, 190, 216]],\n",
       "\n",
       "        [[208, 219, 244],\n",
       "         [205, 216, 240],\n",
       "         [204, 215, 239],\n",
       "         ...,\n",
       "         [175, 191, 217],\n",
       "         [172, 190, 216],\n",
       "         [169, 191, 215]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[207, 199, 181],\n",
       "         [203, 195, 175],\n",
       "         [203, 196, 173],\n",
       "         ...,\n",
       "         [135, 132, 127],\n",
       "         [162, 158, 150],\n",
       "         [168, 163, 151]],\n",
       "\n",
       "        [[198, 190, 170],\n",
       "         [189, 181, 159],\n",
       "         [180, 172, 147],\n",
       "         ...,\n",
       "         [178, 171, 160],\n",
       "         [175, 169, 156],\n",
       "         [175, 169, 154]],\n",
       "\n",
       "        [[198, 189, 173],\n",
       "         [189, 181, 162],\n",
       "         [178, 170, 149],\n",
       "         ...,\n",
       "         [195, 184, 169],\n",
       "         [196, 189, 171],\n",
       "         [195, 190, 171]]],\n",
       "\n",
       "\n",
       "       [[[229, 229, 239],\n",
       "         [236, 237, 247],\n",
       "         [234, 236, 247],\n",
       "         ...,\n",
       "         [217, 219, 233],\n",
       "         [221, 223, 234],\n",
       "         [222, 223, 233]],\n",
       "\n",
       "        [[222, 221, 229],\n",
       "         [239, 239, 249],\n",
       "         [233, 234, 246],\n",
       "         ...,\n",
       "         [223, 223, 236],\n",
       "         [227, 228, 238],\n",
       "         [210, 211, 220]],\n",
       "\n",
       "        [[213, 206, 211],\n",
       "         [234, 232, 239],\n",
       "         [231, 233, 244],\n",
       "         ...,\n",
       "         [220, 220, 232],\n",
       "         [220, 219, 232],\n",
       "         [202, 203, 215]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[150, 143, 135],\n",
       "         [140, 135, 127],\n",
       "         [132, 127, 120],\n",
       "         ...,\n",
       "         [224, 222, 218],\n",
       "         [230, 228, 225],\n",
       "         [241, 241, 238]],\n",
       "\n",
       "        [[137, 132, 126],\n",
       "         [130, 127, 120],\n",
       "         [125, 121, 115],\n",
       "         ...,\n",
       "         [181, 180, 178],\n",
       "         [202, 201, 198],\n",
       "         [212, 211, 207]],\n",
       "\n",
       "        [[122, 119, 114],\n",
       "         [118, 116, 110],\n",
       "         [120, 116, 111],\n",
       "         ...,\n",
       "         [179, 177, 173],\n",
       "         [164, 164, 162],\n",
       "         [163, 163, 161]]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new = data_train[labels_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2500, 32, 32, 3), dtype=float32, numpy=\n",
       "array([[[[ 0.5844234 ,  0.60631365,  0.551588  ],\n",
       "         [ 0.5844234 ,  0.60631365,  0.551588  ],\n",
       "         [ 0.60631365,  0.6282039 ,  0.5734783 ],\n",
       "         ...,\n",
       "         [ 0.63914907,  0.6500942 ,  0.56253314],\n",
       "         [ 0.6172588 ,  0.6282039 ,  0.551588  ],\n",
       "         [ 0.5953685 ,  0.60631365,  0.54064286]],\n",
       "\n",
       "        [[ 0.6282039 ,  0.6500942 ,  0.5953685 ],\n",
       "         [ 0.6282039 ,  0.6500942 ,  0.5953685 ],\n",
       "         [ 0.63914907,  0.66103935,  0.60631365],\n",
       "         ...,\n",
       "         [ 0.6719845 ,  0.68292964,  0.6172588 ],\n",
       "         [ 0.6500942 ,  0.66103935,  0.5953685 ],\n",
       "         [ 0.6282039 ,  0.63914907,  0.5734783 ]],\n",
       "\n",
       "        [[ 0.6719845 ,  0.6938748 ,  0.63914907],\n",
       "         [ 0.68292964,  0.7048199 ,  0.6500942 ],\n",
       "         [ 0.6938748 ,  0.71576506,  0.6719845 ],\n",
       "         ...,\n",
       "         [ 0.71576506,  0.71576506,  0.6719845 ],\n",
       "         [ 0.6938748 ,  0.7048199 ,  0.6500942 ],\n",
       "         [ 0.6719845 ,  0.68292964,  0.6282039 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.7595456 ,  0.7486005 ,  0.80332613],\n",
       "         [ 0.6719845 ,  0.66103935,  0.71576506],\n",
       "         [ 0.4968623 ,  0.4968623 ,  0.54064286],\n",
       "         ...,\n",
       "         [ 1.033174  ,  1.0222288 ,  1.0769546 ],\n",
       "         [ 1.0441191 ,  1.0222288 ,  1.0769546 ],\n",
       "         [ 1.033174  ,  1.033174  ,  1.0769546 ]],\n",
       "\n",
       "        [[ 0.7704907 ,  0.7595456 ,  0.8142713 ],\n",
       "         [ 0.7486005 ,  0.73765534,  0.792381  ],\n",
       "         [ 0.73765534,  0.73765534,  0.78143585],\n",
       "         ...,\n",
       "         [ 1.0112838 ,  1.0003387 ,  1.0550643 ],\n",
       "         [ 1.0112838 ,  1.0003387 ,  1.0550643 ],\n",
       "         [ 1.0112838 ,  1.0003387 ,  1.0550643 ]],\n",
       "\n",
       "        [[ 0.7486005 ,  0.73765534,  0.792381  ],\n",
       "         [ 0.73765534,  0.7267102 ,  0.78143585],\n",
       "         [ 0.7486005 ,  0.73765534,  0.792381  ],\n",
       "         ...,\n",
       "         [ 0.9893935 ,  0.97844833,  1.033174  ],\n",
       "         [ 0.9893935 ,  0.97844833,  1.033174  ],\n",
       "         [ 1.0003387 ,  0.97844833,  1.033174  ]]],\n",
       "\n",
       "\n",
       "       [[[-0.01337779, -0.2604503 , -0.5075228 ],\n",
       "         [-0.13691404, -0.35310248, -0.56929094],\n",
       "         [-0.01337779, -0.22956623, -0.47663873],\n",
       "         ...,\n",
       "         [-0.07514592, -0.2604503 , -0.63105905],\n",
       "         [-0.19868217, -0.38398656, -0.75459534],\n",
       "         [-0.29133436, -0.47663873, -0.8472475 ]],\n",
       "\n",
       "        [[-0.13691404, -0.35310248, -0.600175  ],\n",
       "         [-0.22956623, -0.44575468, -0.69282717],\n",
       "         [-0.1677981 , -0.38398656, -0.63105905],\n",
       "         ...,\n",
       "         [-0.07514592, -0.2604503 , -0.63105905],\n",
       "         [-0.13691404, -0.32221842, -0.69282717],\n",
       "         [-0.29133436, -0.47663873, -0.8472475 ]],\n",
       "\n",
       "        [[-0.13691404, -0.35310248, -0.600175  ],\n",
       "         [-0.1677981 , -0.38398656, -0.63105905],\n",
       "         [-0.1677981 , -0.38398656, -0.63105905],\n",
       "         ...,\n",
       "         [-0.01337779, -0.19868217, -0.56929094],\n",
       "         [-0.10602998, -0.29133436, -0.66194314],\n",
       "         [-0.1677981 , -0.35310248, -0.72371125]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.01337779, -0.2604503 , -0.600175  ],\n",
       "         [-0.01337779, -0.2604503 , -0.600175  ],\n",
       "         [ 0.04839057, -0.19868217, -0.53840685],\n",
       "         ...,\n",
       "         [ 0.20281088, -0.04426185, -0.38398656],\n",
       "         [-0.13691404, -0.38398656, -0.72371125],\n",
       "         [-0.38398656, -0.63105905, -0.97078377]],\n",
       "\n",
       "        [[-0.04426185, -0.29133436, -0.63105905],\n",
       "         [-0.01337779, -0.2604503 , -0.600175  ],\n",
       "         [ 0.01750627, -0.22956623, -0.56929094],\n",
       "         ...,\n",
       "         [ 0.04839057, -0.19868217, -0.53840685],\n",
       "         [-0.1677981 , -0.41487062, -0.75459534],\n",
       "         [-0.38398656, -0.63105905, -0.97078377]],\n",
       "\n",
       "        [[-0.10602998, -0.35310248, -0.69282717],\n",
       "         [-0.10602998, -0.35310248, -0.69282717],\n",
       "         [-0.01337779, -0.2604503 , -0.600175  ],\n",
       "         ...,\n",
       "         [ 0.04839057, -0.22956623, -0.53840685],\n",
       "         [-0.01337779, -0.2604503 , -0.600175  ],\n",
       "         [-0.1677981 , -0.41487062, -0.75459534]]],\n",
       "\n",
       "\n",
       "       [[[ 2.3137074 ,  2.280953  ,  2.2973301 ],\n",
       "         [ 2.2481987 ,  2.2154446 ,  2.2481987 ],\n",
       "         [ 2.2481987 ,  2.2154446 ,  2.2481987 ],\n",
       "         ...,\n",
       "         [ 1.9534107 ,  1.3147032 ,  0.5941102 ],\n",
       "         [ 2.1990674 ,  1.9861649 ,  1.8715252 ],\n",
       "         [ 2.3137074 ,  2.1499362 ,  2.1499362 ]],\n",
       "\n",
       "        [[ 2.280953  ,  2.264576  ,  2.264576  ],\n",
       "         [ 2.2318218 ,  2.1990674 ,  2.1990674 ],\n",
       "         [ 2.2318218 ,  2.2154446 ,  2.1990674 ],\n",
       "         ...,\n",
       "         [ 2.0189192 ,  1.281949  ,  0.5122246 ],\n",
       "         [ 2.280953  ,  2.0844276 ,  2.0189192 ],\n",
       "         [ 2.3300843 ,  2.1663132 ,  2.1499362 ]],\n",
       "\n",
       "        [[ 0.9380296 ,  0.9052754 ,  0.62686443],\n",
       "         [ 0.9380296 ,  0.88889825,  0.62686443],\n",
       "         [ 0.9380296 ,  0.88889825,  0.6432415 ],\n",
       "         ...,\n",
       "         [ 1.7568854 ,  1.1836864 ,  0.4794704 ],\n",
       "         [ 1.7568854 ,  1.6586226 ,  1.5276058 ],\n",
       "         [ 1.8060167 ,  1.6913769 ,  1.6094913 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3812077 ,  0.33207634, -1.1582414 ],\n",
       "         [ 0.21743643,  0.2829449 , -1.2073727 ],\n",
       "         [ 0.23381354,  0.33207634, -1.2237498 ],\n",
       "         ...,\n",
       "         [ 0.13555086,  0.25019065, -1.3056353 ],\n",
       "         [ 0.07004239,  0.20105931, -1.3711438 ],\n",
       "         [ 0.08641951,  0.25019065, -1.3547667 ]],\n",
       "\n",
       "        [[ 0.21743643,  0.25019065, -1.2073727 ],\n",
       "         [ 0.23381354,  0.2829449 , -1.2073727 ],\n",
       "         [ 0.26656777,  0.31569925, -1.2728812 ],\n",
       "         ...,\n",
       "         [ 0.03728816,  0.20105931, -1.3711438 ],\n",
       "         [-0.0282203 ,  0.15192796, -1.3547667 ],\n",
       "         [-0.09372876,  0.15192796, -1.4366523 ]],\n",
       "\n",
       "        [[ 0.25019065,  0.29932213, -1.10911   ],\n",
       "         [ 0.31569925,  0.36483058, -1.0927329 ],\n",
       "         [ 0.26656777,  0.33207634, -1.2073727 ],\n",
       "         ...,\n",
       "         [ 0.13555086,  0.29932213, -1.2237498 ],\n",
       "         [ 0.03728816,  0.20105931, -1.256504  ],\n",
       "         [-0.07735164,  0.18468219, -1.3711438 ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.71864426,  0.8557592 ,  0.9654512 ],\n",
       "         [ 0.5815293 ,  0.77349025,  0.9106052 ],\n",
       "         [ 0.5815293 ,  0.8009132 ,  0.99287415],\n",
       "         ...,\n",
       "         [ 0.14276144,  0.44441435,  0.82833624],\n",
       "         [ 0.08791525,  0.41699135,  0.8009132 ],\n",
       "         [ 0.25245342,  0.6089523 ,  0.99287415]],\n",
       "\n",
       "        [[ 0.36214536,  0.5541063 ,  0.71864426],\n",
       "         [ 0.30729938,  0.5541063 ,  0.7460672 ],\n",
       "         [ 0.41699135,  0.69122124,  0.93802816],\n",
       "         ...,\n",
       "         [-0.13146868,  0.30729938,  0.77349025],\n",
       "         [-0.21373765,  0.2250304 ,  0.69122124],\n",
       "         [ 0.00564628,  0.38956836,  0.88318217]],\n",
       "\n",
       "        [[ 0.52668333,  0.71864426,  0.8557592 ],\n",
       "         [ 0.47183734,  0.71864426,  0.9106052 ],\n",
       "         [ 0.36214536,  0.6363753 ,  0.8557592 ],\n",
       "         ...,\n",
       "         [-0.26858363,  0.17018443,  0.6363753 ],\n",
       "         [-0.3782756 ,  0.08791525,  0.5541063 ],\n",
       "         [-0.3234296 ,  0.08791525,  0.5815293 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.7607179 ,  1.404219  ,  0.33472237],\n",
       "         [ 1.7881409 ,  1.3219501 ,  0.11533845],\n",
       "         [ 1.6784489 ,  1.294527  , -0.13146868],\n",
       "         ...,\n",
       "         [-0.07662269,  0.08791525,  0.03306927],\n",
       "         [-0.02177671,  0.2250304 ,  0.14276144],\n",
       "         [-0.15889166,  0.11533845,  0.00564628]],\n",
       "\n",
       "        [[ 1.6784489 ,  1.3219501 ,  0.25245342],\n",
       "         [ 1.7607179 ,  1.3219501 ,  0.08791525],\n",
       "         [ 1.7058719 ,  1.3493731 , -0.13146868],\n",
       "         ...,\n",
       "         [-0.89931244, -0.7347745 , -0.7347745 ],\n",
       "         [-0.7073515 , -0.5153906 , -0.54281354],\n",
       "         [-0.6525055 , -0.4331216 , -0.48796755]],\n",
       "\n",
       "        [[ 1.8155639 ,  1.486488  ,  0.38956836],\n",
       "         [ 1.8155639 ,  1.486488  ,  0.08791525],\n",
       "         [ 1.7607179 ,  1.404219  , -0.02177671],\n",
       "         ...,\n",
       "         [-0.89931244, -0.7621975 , -0.6799285 ],\n",
       "         [-0.8170434 , -0.6525055 , -0.6250825 ],\n",
       "         [-0.78962046, -0.6525055 , -0.6799285 ]]],\n",
       "\n",
       "\n",
       "       [[[-0.55598897, -0.6084169 , -0.48608506],\n",
       "         [-0.5909409 , -0.64336884, -0.52103704],\n",
       "         [-0.57346493, -0.6258929 , -0.503561  ],\n",
       "         ...,\n",
       "         [-0.538513  , -0.57346493, -0.52103704],\n",
       "         [-0.538513  , -0.57346493, -0.52103704],\n",
       "         [-0.538513  , -0.57346493, -0.52103704]],\n",
       "\n",
       "        [[-0.55598897, -0.6084169 , -0.48608506],\n",
       "         [-0.5909409 , -0.64336884, -0.52103704],\n",
       "         [-0.57346493, -0.6258929 , -0.503561  ],\n",
       "         ...,\n",
       "         [-0.48608506, -0.52103704, -0.4686091 ],\n",
       "         [-0.503561  , -0.538513  , -0.48608506],\n",
       "         [-0.538513  , -0.57346493, -0.52103704]],\n",
       "\n",
       "        [[-0.57346493, -0.6258929 , -0.503561  ],\n",
       "         [-0.57346493, -0.6258929 , -0.503561  ],\n",
       "         [-0.55598897, -0.6084169 , -0.48608506],\n",
       "         ...,\n",
       "         [-0.48608506, -0.52103704, -0.4686091 ],\n",
       "         [-0.48608506, -0.52103704, -0.4686091 ],\n",
       "         [-0.52103704, -0.55598897, -0.503561  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.52103704, -0.57346493, -0.45113313],\n",
       "         [-0.503561  , -0.55598897, -0.43365714],\n",
       "         [-0.45113313, -0.503561  , -0.38122916],\n",
       "         ...,\n",
       "         [-0.48608506, -0.5909409 , -0.52103704],\n",
       "         [-0.48608506, -0.5909409 , -0.52103704],\n",
       "         [-0.48608506, -0.5909409 , -0.52103704]],\n",
       "\n",
       "        [[-0.538513  , -0.5909409 , -0.4686091 ],\n",
       "         [-0.52103704, -0.57346493, -0.45113313],\n",
       "         [-0.503561  , -0.55598897, -0.43365714],\n",
       "         ...,\n",
       "         [-0.503561  , -0.6084169 , -0.538513  ],\n",
       "         [-0.503561  , -0.6084169 , -0.538513  ],\n",
       "         [-0.538513  , -0.64336884, -0.57346493]],\n",
       "\n",
       "        [[-0.538513  , -0.5909409 , -0.4686091 ],\n",
       "         [-0.52103704, -0.57346493, -0.45113313],\n",
       "         [-0.503561  , -0.55598897, -0.43365714],\n",
       "         ...,\n",
       "         [-0.52103704, -0.6084169 , -0.538513  ],\n",
       "         [-0.52103704, -0.6258929 , -0.55598897],\n",
       "         [-0.538513  , -0.64336884, -0.57346493]]],\n",
       "\n",
       "\n",
       "       [[[ 0.75095606,  0.75095606,  0.75095606],\n",
       "         [ 0.70492554,  0.70492554,  0.70492554],\n",
       "         [ 0.70492554,  0.70492554,  0.70492554],\n",
       "         ...,\n",
       "         [ 0.6819103 ,  0.6819103 ,  0.6819103 ],\n",
       "         [ 0.6473873 ,  0.6473873 ,  0.6473873 ],\n",
       "         [ 0.6934179 ,  0.6934179 ,  0.6934179 ]],\n",
       "\n",
       "        [[ 0.76246375,  0.76246375,  0.76246375],\n",
       "         [ 0.76246375,  0.76246375,  0.76246375],\n",
       "         [ 0.76246375,  0.76246375,  0.76246375],\n",
       "         ...,\n",
       "         [ 0.1525589 ,  0.1525589 ,  0.1525589 ],\n",
       "         [ 0.6013568 ,  0.6013568 ,  0.6013568 ],\n",
       "         [ 0.7394484 ,  0.7394484 ,  0.7394484 ]],\n",
       "\n",
       "        [[ 0.76246375,  0.76246375,  0.76246375],\n",
       "         [ 0.75095606,  0.75095606,  0.75095606],\n",
       "         [ 0.75095606,  0.75095606,  0.75095606],\n",
       "         ...,\n",
       "         [-1.4930335 , -1.4930335 , -1.4930335 ],\n",
       "         [-0.43433067, -0.43433067, -0.43433067],\n",
       "         [ 0.6358797 ,  0.6358797 ,  0.6358797 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.76246375,  0.76246375,  0.76246375],\n",
       "         [ 0.75095606,  0.75095606,  0.75095606],\n",
       "         [ 0.76246375,  0.76246375,  0.76246375],\n",
       "         ...,\n",
       "         [ 0.65889496,  0.65889496,  0.65889496],\n",
       "         [ 0.6704026 ,  0.6704026 ,  0.6704026 ],\n",
       "         [ 0.75095606,  0.75095606,  0.75095606]],\n",
       "\n",
       "        [[ 0.76246375,  0.76246375,  0.76246375],\n",
       "         [ 0.76246375,  0.76246375,  0.76246375],\n",
       "         [ 0.76246375,  0.76246375,  0.76246375],\n",
       "         ...,\n",
       "         [ 0.6704026 ,  0.6704026 ,  0.6704026 ],\n",
       "         [ 0.6704026 ,  0.6704026 ,  0.6704026 ],\n",
       "         [ 0.76246375,  0.76246375,  0.76246375]],\n",
       "\n",
       "        [[ 0.76246375,  0.76246375,  0.76246375],\n",
       "         [ 0.7279408 ,  0.7279408 ,  0.7279408 ],\n",
       "         [ 0.7279408 ,  0.7279408 ,  0.7279408 ],\n",
       "         ...,\n",
       "         [ 0.6358797 ,  0.6358797 ,  0.6358797 ],\n",
       "         [ 0.6358797 ,  0.6358797 ,  0.6358797 ],\n",
       "         [ 0.7279408 ,  0.7279408 ,  0.7279408 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length=int((len(x_train_new))/2)\n",
    "x_train_new[0:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, BatchNormalization\n",
    "vall=Flatten()(x_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3072,), dtype=float32, numpy=\n",
       "array([0.5844234 , 0.60631365, 0.551588  , ..., 1.0003387 , 0.97844833,\n",
       "       1.033174  ], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture.append([data_input_shape[0],3]) # input information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture.append([3,10]) # Upsampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train=np.squeeze(labels_train,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network=Final_Network(hyperp, run_options, data_input_shape, label_dimensions,\n",
    "                      kernel_regularizer, bias_regularizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network.load_weights(\"WEIGHTS\"+'/'+\"model_weights1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_train, labels_train), (data_test, labels_test) = datasets.mnist.load_data()\n",
    "data_train = data_train.reshape(data_train.shape[0], 28, 28, 1)\n",
    "data_test = data_test.reshape(data_test.shape[0], 28, 28, 1)\n",
    "label_dimensions = 10\n",
    "data_input_shape = data_train.shape[1:]\n",
    "\n",
    "#=== Casting as float32 ===#\n",
    "data_train = tf.cast(data_train,tf.float32)\n",
    "labels_train = tf.cast(labels_train, tf.int32)\n",
    "data_test = tf.cast(data_test, tf.float32)\n",
    "labels_test = tf.cast(labels_test, tf.int32)\n",
    "    \n",
    "#=== Normalize Data ===#\n",
    "data_train, data_test = data_train/255.0, data_test/255.0\n",
    "data_train = tf.image.per_image_standardization(data_train) # Linearly scales each image to have mean 0 and variance 1\n",
    "data_test = tf.image.per_image_standardization(data_test)   # Linearly scales each image to have mean 0 and variance 1\n",
    "\n",
    "data_train = tf.reshape(data_train, (len(data_train), 28*28))\n",
    "data_test = tf.reshape(data_test, (len(data_test), 28*28))\n",
    "\n",
    "num_data_train = len(data_train)\n",
    "num_data_test = len(data_test)\n",
    "random_seed=1234\n",
    "batch_size        = 1000\n",
    "\n",
    "data_and_labels_train_full = tf.data.Dataset.from_tensor_slices((data_train, labels_train)).shuffle(num_data_train, seed=random_seed)\n",
    "data_and_labels_test = tf.data.Dataset.from_tensor_slices((data_test, labels_test)).batch(batch_size)\n",
    "num_batches_test = len(list(data_and_labels_test))\n",
    "\n",
    "#=== Partitioning Out Validation Set and Constructing Batches ===#\n",
    "current_num_data_train = num_data_train\n",
    "num_data_train = int(0.8 * num_data_train)\n",
    "num_data_val = current_num_data_train - num_data_train\n",
    "data_and_labels_train_full = tf.data.Dataset.from_tensor_slices((data_train, labels_train)).shuffle(num_data_train, seed=random_seed)\n",
    "data_and_labels_train = data_and_labels_train_full.take(num_data_train).batch(batch_size)\n",
    "data_and_labels_val = data_and_labels_train_full.skip(num_data_train).batch(batch_size)    \n",
    "num_batches_train = len(list(data_and_labels_train))\n",
    "num_batches_val = len(list(data_and_labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_train, labels_train), (data_test, labels_test) = datasets.mnist.load_data()\n",
    "data_train = data_train.reshape(data_train.shape[0], 28, 28, 1)\n",
    "data_test = data_test.reshape(data_test.shape[0], 28, 28, 1)\n",
    "label_dimensions = 10\n",
    "data_input_shape = data_train.shape[1:]\n",
    "\n",
    "#=== Casting as float32 ===#\n",
    "data_train = tf.cast(data_train,tf.float32)\n",
    "labels_train = tf.cast(labels_train, tf.int32)\n",
    "data_test = tf.cast(data_test, tf.float32)\n",
    "labels_test = tf.cast(labels_test, tf.int32)\n",
    "    \n",
    "#=== Normalize Data ===#\n",
    "data_train, data_test = data_train/255.0, data_test/255.0\n",
    "data_train = tf.image.per_image_standardization(data_train) # Linearly scales each image to have mean 0 and variance 1\n",
    "data_test = tf.image.per_image_standardization(data_test)   # Linearly scales each image to have mean 0 and variance 1\n",
    "\n",
    "data_train = tf.reshape(data_train, (len(data_train), 28*28))\n",
    "data_test = tf.reshape(data_test, (len(data_test), 28*28))\n",
    "\n",
    "num_data_train = len(data_train)\n",
    "num_data_test = len(data_test)\n",
    "random_seed=1234\n",
    "batch_size        = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=tf.one_hot(tf.cast(labels_train,tf.int64), label_dimensions, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_labels_train_full = tf.data.Dataset.from_tensor_slices((data_train, labels_train,labels)).shuffle(num_data_train, seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_labels_train = data_and_labels_train_full.take(num_data_train).batch(batch_size)\n",
    "data_and_labels_val = data_and_labels_train_full.skip(num_data_train).batch(batch_size)    \n",
    "num_batches_train = len(list(data_and_labels_train))\n",
    "num_batches_val = len(list(data_and_labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_num, (batch_data_train, batch_labels_train,labels) in data_and_labels_train.enumerate():\n",
    "    batch=batch_num\n",
    "    batch_data_train = batch_data_train\n",
    "    batch_labels_train=batch_labels_train\n",
    "    lab=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.one_hot(tf.cast(labels_train,tf.int64), label_dimensions, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tf.keras.losses.mean_squared_error(new_one, val[0:dimension[0]-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_new = batch_data_train[batch_labels_train == 1]\n",
    "batch_pred_train,val=NN(x_train_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.one_hot(tf.cast(y_true,tf.int64), label_dimensions, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(val[0]-val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
